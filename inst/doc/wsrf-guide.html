<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="He Zhao, Graham Williams" />

<meta name="date" content="2025-12-16" />

<title>A Quick Start Guide for wsrf</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/rstudio/markdown/inst/resources/prism-xcode.css" data-external="1">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/yihui/knitr/inst/misc/vignette.css" data-external="1">
<script src="https://cdn.jsdelivr.net/combine/npm/@xiee/utils/js/code-lang.min.js,npm/@xiee/utils/js/number-captions.min.js,npm/prismjs@1.29.0/components/prism-core.min.js" data-external="1" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" data-external="1" defer></script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>





</head>

<body>




<h1 class="title toc-ignore">A Quick Start Guide for wsrf</h1>
<h4 class="author">He Zhao, Graham Williams</h4>
<h4 class="date">2025-12-16</h4>


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#requirements-and-installation-notes" id="toc-requirements-and-installation-notes">Requirements and
Installation Notes</a></li>
<li><a href="#usage" id="toc-usage">Usage</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The <a href="https://cran.r-project.org/package=wsrf"><strong>wsrf</strong></a>
package is a parallel implementation of the Weighted Subspace Random
Forest algorithm (wsrf) of <span class="citation">Xu et al.
(2012)</span>. A novel variable weighting method is used for variable
subspace selection in place of the traditional approach of random
variable sampling. This new approach is particularly useful in building
models for high dimensional data — often consisting of thousands of
variables. Parallel computation is used to take advantage of multi-core
machines and clusters of machines to build random forest models from
high dimensional data with reduced elapsed times.</p>
</div>
<div id="requirements-and-installation-notes" class="section level2">
<h2>Requirements and Installation Notes</h2>
<p>Currently, <strong>wsrf</strong> requires R (&gt;= 3.3.0), <a href="https://cran.r-project.org/package=Rcpp"><strong>Rcpp</strong></a>
(&gt;= 0.10.2) <span class="citation">(Eddelbuettel and François 2011;
Eddelbuettel 2013)</span>. For the use of multi-threading, a C++
compiler with <a href="https://en.wikipedia.org/wiki/C%2B%2B11">C++11</a> standard
support of threads is required. To install the latest stable version of
the package, from within R run:</p>
<pre class="r"><code>install.packages(&quot;wsrf&quot;)</code></pre>
<p>or the latest development version:</p>
<pre class="r"><code>devtools::install_github(&quot;simonyansenzhao/wsrf&quot;)</code></pre>
<p>The version of R before 3.3.0 doesn’t provide fully support of C++11,
thus we provided other options for installation of wsrf. From 1.6.0, we
drop the support for those options. One can find the usage in the
documentation from previous version if interested.</p>
</div>
<div id="usage" class="section level2">
<h2>Usage</h2>
<p>This section demonstrates how to use <strong>wsrf</strong>,
especially on a cluster of machines.</p>
<p>The example uses a small dataset <em>iris</em> from R. See the help
page in R (<code>?iris</code>) for more details of <em>iris</em>. Below
are the basic information of it.</p>
<pre class="r"><code>ds &lt;- iris
dim(ds)</code></pre>
<pre><code>## [1] 150   5</code></pre>
<pre class="r"><code>names(ds)</code></pre>
<pre><code>## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot;  &quot;Species&quot;</code></pre>
<p>Before building the model we need to prepare the training dataset.
First we specify the target variable.</p>
<pre class="r"><code>target &lt;- &quot;Species&quot;
vars &lt;- names(ds)</code></pre>
<p>Next we deal with missing values, using <code>na.roughfix()</code>
from <strong>randomForest</strong> to take care of them.</p>
<pre class="r"><code>library(&quot;randomForest&quot;)
if (sum(is.na(ds[vars]))) ds[vars] &lt;- na.roughfix(ds[vars])
ds[target] &lt;- as.factor(ds[[target]])
(tt &lt;- table(ds[target]))</code></pre>
<pre><code>## Species
##     setosa versicolor  virginica 
##         50         50         50</code></pre>
<p>We construct the formula that describes the model which will predict
the target based on all other variables.</p>
<pre class="r"><code>(form &lt;- as.formula(paste(target, &quot;~ .&quot;)))</code></pre>
<pre><code>## Species ~ .</code></pre>
<p>Finally we create the randomly selected training and test datasets,
setting a seed so that the results can be exactly replicated.</p>
<pre class="r"><code>seed &lt;- 42
set.seed(seed)
length(train &lt;- sample(nrow(ds), 0.7*nrow(ds)))</code></pre>
<pre><code>## [1] 105</code></pre>
<pre class="r"><code>length(test &lt;- setdiff(seq_len(nrow(ds)), train))</code></pre>
<pre><code>## [1] 45</code></pre>
<p>The function to build a weighted random forest model in
<strong>wsrf</strong> is:</p>
<pre class="r"><code>wsrf(formula, data, ...)</code></pre>
<p>and</p>
<pre class="r"><code>wsrf(x,
     y,
     mtry=floor(log2(length(x))+1),
     ntree=500,
     weights=TRUE,
     parallel=TRUE,
     na.action=na.fail,
     importance=FALSE,
     nodesize=2,
     clusterlogfile,
     ...)</code></pre>
<p>We use the training dataset to build a random forest model. All
parameters, except <code>formula</code> and <code>data</code>, use their
default values: <code>500</code> for <code>ntree</code> — the number of
trees; <code>TRUE</code> for <code>weights</code> — weighted subspace
random forest or random forest; <code>TRUE</code> for
<code>parallel</code> — use multi-thread or other options, etc.</p>
<pre class="r"><code>library(&quot;wsrf&quot;)
model.wsrf.1 &lt;- wsrf(form, data=ds[train, vars], parallel=FALSE)
print(model.wsrf.1)</code></pre>
<pre><code>## A Weighted Subspace Random Forest model with 500 trees.
## 
##   No. of variables tried at each split: 3
##         Minimum size of terminal nodes: 2
##                  Out-of-Bag Error Rate: 0.08
##                               Strength: 0.84
##                            Correlation: 0.10
## 
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         37          1         0        0.03
## versicolor      0         33         2        0.06
## virginica       0          5        27        0.16</code></pre>
<pre class="r"><code>print(model.wsrf.1, 1)  # Print tree 1.</code></pre>
<pre><code>## Tree 1 has 4 tests (internal nodes), with OOB error rate 0.1000:
## 
##  1) Petal.Width &lt;= 0.5   [setosa] (1 0 0) *
##  1) Petal.Width &gt;  0.5
##  .. 2) Petal.Width &lt;= 1.6
##  .. .. 3) Petal.Length &lt;= 4.9   [versicolor] (0 1 0) *
##  .. .. 3) Petal.Length &gt;  4.9   [versicolor] (0 0.5 0.5) *
##  .. 2) Petal.Width &gt;  1.6
##  .. .. 4) Petal.Length &lt;= 4.8   [versicolor] (0 1 0) *
##  .. .. 4) Petal.Length &gt;  4.8   [virginica] (0 0 1) *</code></pre>
<p>Then, <code>predict</code> the classes of test data.</p>
<pre class="r"><code>cl &lt;- predict(model.wsrf.1, newdata=ds[test, vars], type=&quot;class&quot;)$class
actual &lt;- ds[test, target]
(accuracy.wsrf &lt;- mean(cl == actual, na.rm=TRUE))</code></pre>
<pre><code>## [1] 0.9555556</code></pre>
<p>Thus, we have built a model that is around 96% accurate on unseen
testing data.</p>
<p>Using different random seed, we obtain another model.</p>
<pre class="r"><code>set.seed(seed+1)

# Here we build another model without weighting.
model.wsrf.2 &lt;- wsrf(form, data=ds[train, vars], weights=FALSE, parallel=FALSE)
print(model.wsrf.2)</code></pre>
<pre><code>## A Weighted Subspace Random Forest model with 500 trees.
## 
##   No. of variables tried at each split: 3
##         Minimum size of terminal nodes: 2
##                  Out-of-Bag Error Rate: 0.07
##                               Strength: 0.85
##                            Correlation: 0.08
## 
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         38          0         0        0.00
## versicolor      0         33         2        0.06
## virginica       0          5        27        0.16</code></pre>
<p>We can also derive a subset of the forest from the model or a
combination of multiple forests.</p>
<pre class="r"><code>submodel.wsrf &lt;- subset.wsrf(model.wsrf.1, 1:150)
print(submodel.wsrf)</code></pre>
<pre><code>## A Weighted Subspace Random Forest model with 150 trees.
## 
##   No. of variables tried at each split: 3
##         Minimum size of terminal nodes: 2
##                  Out-of-Bag Error Rate: 0.09
##                               Strength: 0.84
##                            Correlation: 0.10
## 
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         36          2         0        0.05
## versicolor      0         33         2        0.06
## virginica       0          5        27        0.16</code></pre>
<pre class="r"><code>bigmodel.wsrf &lt;- combine.wsrf(model.wsrf.1, model.wsrf.2)
print(bigmodel.wsrf)</code></pre>
<pre><code>## A Weighted Subspace Random Forest model with 1000 trees.
## 
##   No. of variables tried at each split: 3
##         Minimum size of terminal nodes: 2
##                  Out-of-Bag Error Rate: 0.08
##                               Strength: 0.84
##                            Correlation: 0.08
## 
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         37          1         0        0.03
## versicolor      0         33         2        0.06
## virginica       0          5        27        0.16</code></pre>
<p>Next, we will specify building the model on a cluster of servers.</p>
<pre class="r"><code>servers &lt;- paste0(&quot;node&quot;, 31:40)
model.wsrf.3 &lt;- wsrf(form, data=ds[train, vars], parallel=servers)</code></pre>
<p>All we need is a character vector specifying the hostnames of which
nodes to use, or a named integer vector, whose values of the elements
give how many threads to use for model building, in other words, how
many trees built simultaneously. More detail descriptions about
<strong>wsrf</strong> are presented in the <a href="https://cran.r-project.org/package=wsrf/wsrf.pdf">manual</a>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-dirk2013seamless" class="csl-entry">
Eddelbuettel, Dirk. 2013. <em>Seamless and Integration with </em>. New
York: Springer.
</div>
<div id="ref-dirk2011rcpp" class="csl-entry">
Eddelbuettel, Dirk, and Romain François. 2011. <span>“: Seamless and
Integration.”</span> <em>Journal of Statistical Software</em> 40 (8):
1–18. <a href="https://doi.org/10.18637/jss.v040.i08">https://doi.org/10.18637/jss.v040.i08</a>.
</div>
<div id="ref-xu2012classifying" class="csl-entry">
Xu, Baoxun, Joshua Zhexue Huang, Graham Williams, Qiang Wang, and
Yunming Ye. 2012. <span>“Classifying Very High-Dimensional Data with
Random Forests Built from Small Subspaces.”</span> <em>International
Journal of Data Warehousing and Mining (IJDWM)</em> 8 (2): 44–63.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
